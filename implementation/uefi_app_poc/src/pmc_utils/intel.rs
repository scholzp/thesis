
use log::info;
use x86::cpuid::CpuId;
use x86::msr::{
	rdmsr, wrmsr,
	MSR_OFFCORE_RSP_0, MSR_OFFCORE_RSP_1,
	IA32_PERFEVTSEL0, IA32_A_PMC0,
	IA32_PERFEVTSEL1, IA32_A_PMC1,
	IA32_PERFEVTSEL2, IA32_A_PMC2,
	IA32_PERFEVTSEL3, IA32_A_PMC3,
};
use x86_64::instructions::nop;
use alloc::vec::Vec;
use crate::pmc_utils::vendor;

/// https://perfmon-events.intel.com/
/// Supported architectures: Skylake

/// Bitmask to select MSR_OFFCORE_RSP0 as offcore event configuration provider
const OFFCORE_RSP0_EVENT_CODE: u64 = 0xb7;
/// Bitmask to select MSR_OFFCORE_RSP1 as offcore event configuration provider
const OFFCORE_RSP1_EVENT_CODE: u64 = 0xbb;
/// UMASK for offcore events
const OFFCORE_RSP_UNIT_MASK: u64 = 0x01 << 8;

/// Counts the number of demand data reads and page table entry cacheline reads.
/// Does not count hw or sw prefetches.
const REQUEST_DMND_DATA_RD: u64 = 0x1 <<  0;
/// Counts the number of demand reads for ownership (RFO) requests generated by
/// a write to data cacheline. Does not count L2 RFO prefetches.
const REQUEST_DMND_RFO: u64 = 0x1 <<  1;
/// Counts the number of demand instruction cacheline reads and L1 instruction
/// cacheline prefetches.
const REQUEST_DMND_IFETCH: u64 = 0x1 <<  2;
/// Counts miscellaneous requests, such as I/O and uncacheable accesses.
const REQUEST_OTHER: u64 = 0x1 << 15;

/// Catch all value for any response types.
const SUPPLIER_ANY: u64 = 0x1 << 16;
/// No Supplier Information available.
const SUPPLIER_NO_SUPP: u64 = 0x1 << 17;
/// M-state initial lookup stat in L3.
const SUPPLIER_L3_HITM_SATE: u64 = 0x1 << 18;
/// E-state initial lookup stat in L3.
const SUPPLIER_L3_HIT_E_SATE: u64 = 0x1 << 19;
/// S-state initial lookup stat in L3.
const SUPPLIER_L3_HIT_S_SATE: u64 = 0x1 << 20;
/// L4 Cache hit
const SUPPLIER_L4_HIT: u64 = 0x1 << 22;
/// Local Node
const SUPPLIER_DRAM: u64 = 0x1 << 26;
/// L4 cache super line hit (if L4 present)
const SUPPLIER_SPL_HIT: u64 = 0x1 << 30;

/// No details on snoop-related information.
const SNOOP_NONE: u64 = 0x1 << 31;
/// No snoop was needed to satisfy the request.
const SNOOP_NOT_NEEDED: u64 = 0x1 << 32;
/// A snoop was needed and it missed all snooped caches
const SNOOP_MISS: u64 = 0x1 << 33;
/// A snoop was needed and it hits in at least one snooped cache. Hit denotes a
/// cache-line was valid before snoop effect.
const SNOOP_HIT_NO_FWD: u64 = 0x1 << 34;
/// A snoop was needed and data was forwarded from a remote socket.
const SNOOP_HIT_WITH_FWD: u64 = 0x1 << 35;
/// A snoop was needed and it HitM-ed in local or remote cache. HitM denotes a
/// cache-line was in modified state before effect as a results of snoop. 
const SNOOP_HITM: u64 = 0x1 << 36;
/// Target was non-DRAM system address. This includes MMIO transactions.
const SNOOP_NO_DRAM: u64 = 0x1 << 37;

/// USR bit in PERFEVTSEL. When set, counter is incremented when logical core is
/// in privilege level 1,2 or 3.
const  IA32_PERFEVTSEL_USR: u64 = 0x1 << 16;
/// OS bit of PERFEVTSEL. When set, counter is incremented when logical core is
/// in privilege level 0.
const  IA32_PERFEVTSEL_OS: u64 = 0x1 << 17;
/// E bit in PERFEVTSEL. Enables (when set) edge detection of the selected 
/// microarchitectural condition.
const  IA32_PERFEVTSEL_E: u64 = 0x1 << 18;
/// PC bit in PERFEVTSEL. Not supported since Sandy Bridge (Core 2xxx). When set
/// processor toggles PMi pins and increments the PMC. When clear, processor 
/// toggles PMi pins on counter overflow
const  IA32_PERFEVTSEL_PC: u64 = 0x1 << 19;
/// When set, the logical processor generates an exception through its local
/// APIC on counter overflow
const  IA32_PERFEVTSEL_INT: u64 = 0x1 << 20;
/// When set the corresponding PMC counts the event. When clear, the counting 
/// stops and the corresponding PMC can be written
const  IA32_PERFEVTSEL_EN: u64 = 0x1 << 22;
/// Invert flag. Inverts counter mask when set.
const  IA32_PERFEVTSEL_INV: u64 = 0x1 << 23;

/// Contains bits enabling the events counting Snoop responses Multiple bits can
/// be set to true. Occupies bit 31..37 in MSR_OFFCORE_RSP_x.
struct MsrOffcoreRspEventCounter {
	index: u8,
	pmc_index: u8,
	content: u64,
}

impl Default for MsrOffcoreRspEventCounter {
	fn default() -> Self {
		Self {
			index: 0x0_u8,
			pmc_index: 0x0_u8,
			content: 0x0_u64,
		}
	} 
}

impl MsrOffcoreRspEventCounter {
	/// Creates new MsrOffcoreRspConfig with given id.
	/// 
	/// A processor can implement multiple MSR_OFFCORE_RSP registers. In this 
	/// case they are denoted MSR_OFFCORE_RSP_x in the Intel SDM.
	/// 
	/// * `x`			- Index of the MSR_OFFCORE_RSP to use
	/// * `pmc_index`	- Index of the GP performance monitoring register to use
	pub fn new(x: u8, pmc_index: u8) -> Self {
		Self {
			index: x,
			pmc_index: pmc_index,
			content: 0x0_u64,
		}
	}

	/// Updates the configuration stored in this struct.
	/// 
	/// This does not automatically write to the respective MSR
	/// 
	/// * `config`- Bitvector to use for later operations
	pub fn set_offcore_configuration(&mut self, config: u64) {
		self.content = config;
	}

	/// Sets index.
	/// 
	/// * `x`- Index of the MSR_OFFCORE_RSP to use
	pub fn set_index(&mut self, x: u8) {
		self.index = x;
	}

	/// Initialize and activate the counter facility.
	/// 
	/// Write the configuration to the MSR_OFFCORE_RSP and activate the 
	/// respective GP PMC to count events using this configuration. Reset the 
	/// counter to the given value.
	/// 
	/// 
	/// * `init_v`: Value to reset the counter to
	pub fn activate_counter(&self, init_v: u64) {
		/* To activate a offcore PMC, we need to do the following things:
		*  1) Configure the MSR_OFFCORE_RSPx with the actual event configuration
		*  2) Configure the IA32_PERFEVTSELx with the behavior we wish for
		*  3) Event and UMASK in IA32_PERFEVTSELx are chosen so that the 
		*     PMC uses the configuration from MSR_OFFCORE_RSPx
		*  4) Initialize IA32_PMCx (do we increment, do we decrement...?)
		*  5) Start the counter by setting the bit in IA32_PERFEVTSELx
		*/
		let mut event_code: u64 = 0x0;
		// 1) Find the MSR_OFFCORE_RSPx to use and write the configuration to
		//    it
		match self.index {
			0 => unsafe {
				wrmsr(MSR_OFFCORE_RSP_0, self.content);
				event_code = OFFCORE_RSP0_EVENT_CODE;
				info!("Wrote to MSR_OFFCORE_RSP_0: {:#016x?}", self.content);
			},
			1 => unsafe {
				wrmsr(MSR_OFFCORE_RSP_1, self.content);
				event_code = OFFCORE_RSP1_EVENT_CODE; 
			},
			_ => return, //TODO: We want, at some point, return an error
		}
		// We cant to count all occurences (OS and User) of the eventcode of the
		// chosen OFFCORE_RSP
		let perfsel_content = 0x0_u64 
		| IA32_PERFEVTSEL_USR | IA32_PERFEVTSEL_OS // count in all priv levels
		| event_code			// Event depending on chosen MSR_OFFCORE_RSPx
		| OFFCORE_RSP_UNIT_MASK // offcore event UMASK
		| IA32_PERFEVTSEL_EN; 	// Start the counter
		// 2 & 3 & 4 & 5) Depending on the index of the PMC we do the same thing
		match self.pmc_index {
			0 => {
				// We operate on PMC0 and IA32_PERFEVTSEL0
				Self::init_and_conf_pmc(
					IA32_PERFEVTSEL0, 
					IA32_A_PMC0, 
					init_v,
					perfsel_content
				);
			},
			1 => {
				// We operate on PMC1 and IA32_PERFEVTSEL1
				Self::init_and_conf_pmc(
					IA32_PERFEVTSEL1,
					IA32_A_PMC1,
					init_v,
					perfsel_content
				);
			},
			2 => {
				// We operate on PMC2 and IA32_PERFEVTSEL2
				Self::init_and_conf_pmc(
					IA32_PERFEVTSEL2, 
					IA32_A_PMC2, 
					init_v,
					perfsel_content
				);
				
			},
			3 => {
				// We operate on PMC3 and IA32_PERFEVTSEL3
				Self::init_and_conf_pmc(
					IA32_PERFEVTSEL3, 
					IA32_A_PMC3, 
					init_v,
					perfsel_content
				);
				
			},
			_ => {
				info!("No CPU known to implement 4 or more GP PMCs!");
				return;  //TODO: We want, at some point, return an error
			},
		}
	}

	fn init_and_conf_pmc(perfevtsel_register: u32, pmc_register: u32, init_v: u64, perfsel_content: u64) {
		unsafe { 
			// Cancel any running performance measurements
			wrmsr(perfevtsel_register, 0x0_u64);
			// Reset the counter to zero
			wrmsr(pmc_register, init_v);
			// MSR_OFFCOREx was configured before
			// Activate the counter
			wrmsr(perfevtsel_register, perfsel_content);
		}
	}

	pub fn read_pcm_val(&self) -> u64 {
		match self.pmc_index {
			0 => unsafe { rdmsr(IA32_A_PMC0) },
			1 => unsafe { rdmsr(IA32_A_PMC1) },
			2 => unsafe { rdmsr(IA32_A_PMC2) },
			3 => unsafe { rdmsr(IA32_A_PMC3) },
			_ => {
				info!("No CPU known to implement 4 or more GP PMCs!");
				//return;  //TODO: We want, at some point, return an error
				0
			},
		}
	}
}

/// Returns performance monitoring related features of th CPU
pub fn query_features_intel() {
	if false == vendor::check_vendor(vendor::CpuVendor::Intel) {
		return
	}
	let cpuid = CpuId::new();
	info!("{:?}", cpuid.get_performance_monitoring_info().unwrap());
}

pub fn test_offcore_pmc() {
	let mut counter = MsrOffcoreRspEventCounter::new(0, 0);

	if false == vendor::check_vendor(vendor::CpuVendor::Intel) {
		return
	}

	counter.set_offcore_configuration(
		0x0_u64 | SUPPLIER_ANY | SUPPLIER_DRAM | SUPPLIER_NO_SUPP
		| SUPPLIER_L3_HIT_E_SATE | SUPPLIER_L3_HIT_S_SATE
		| SUPPLIER_L3_HITM_SATE
		| REQUEST_DMND_DATA_RD | REQUEST_DMND_IFETCH
		| REQUEST_DMND_RFO | REQUEST_OTHER | SNOOP_NOT_NEEDED | SNOOP_HITM |
		SNOOP_HIT_NO_FWD | SNOOP_HIT_WITH_FWD | SNOOP_MISS | SNOOP_NONE
	);
	counter.activate_counter(0x0_u64);

	let mut vec = Vec::new();
	

	let num_vec_elements : u64 = 0x1_u64 << 14;
	for x in 0..num_vec_elements {
		vec.push(x);
	}
	info!("Initialized vector");

	for x in 0..num_vec_elements {
		vec.push(x);
	}
	vec = vec.iter().map(|x | x * x).collect();

	info!("Modified vector, current size = {:?}, at least {:?} bytes", 
		vec.len(), vec.len() 
	);
} 